{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, dot, conj, prod, sqrt, exp, pi, diag, angle, array, argwhere, real, floor, frombuffer, uint8, where, stack, asarray\n",
    "from numpy.linalg import qr, multi_dot, svd\n",
    "from numpy.random import uniform, normal, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, pls_params):\n",
    "        \"\"\"\n",
    "        Initialization of class\n",
    "        :param pls_params: object from PLSParameters class containing basic parameters\n",
    "        \"\"\"\n",
    "        self.bandwidth = pls_params.bandwidth\n",
    "        self.bin_spacing = pls_params.bin_spacing\n",
    "        self.num_ant = pls_params.num_ant\n",
    "        self.bit_codebook = pls_params.bit_codebook\n",
    "\n",
    "        self.NFFT = pls_params.NFFT\n",
    "        self.num_used_bins = pls_params.num_used_bins\n",
    "        self.subband_size = self.num_ant\n",
    "\n",
    "        self.num_subbands = pls_params.num_subbands\n",
    "        self.num_PMI = self.num_subbands\n",
    "\n",
    "        self.key_len = self.num_subbands * self.bit_codebook\n",
    "\n",
    "    def unitary_gen(self):\n",
    "        \"\"\"\n",
    "        Generate random nitary matrices for each sub-band\n",
    "        :return GA: Unitary matrices in each sub-band at Alice\n",
    "        \"\"\"\n",
    "        GA = zeros(self.num_subbands, dtype=object)\n",
    "        for sb in range(0, self.num_subbands):\n",
    "            Q, R = qr(uniform(0, 1, (self.num_ant, self.num_ant))\n",
    "                      +1j*uniform(0, 1, (self.num_ant, self.num_ant)))\n",
    "\n",
    "            GA[sb] = dot(Q, diag(diag(R)/abs(diag(R))))\n",
    "        return GA\n",
    "\n",
    "    @staticmethod\n",
    "    def awgn(in_signal, SNRdB):\n",
    "        \"\"\"\n",
    "        Adds AWGN to the input signal. Maintains a given SNR.\n",
    "        :param in_signal: input signal to which noise needs to be addded\n",
    "        :param SNRdB: Signal to Noise Ratio in dB\n",
    "        :return: noisy signal\n",
    "        \"\"\"\n",
    "        S0 = in_signal*conj(in_signal)\n",
    "        S = S0.sum() / prod(in_signal.shape)\n",
    "        SNR = 10 ** (SNRdB / 10)\n",
    "        N = S.real / SNR\n",
    "        awg_noise = sqrt(N / 2) * normal(0, 1, in_signal.shape) + \\\n",
    "                    1j * sqrt(N / 2) * normal(0, 1, in_signal.shape)\n",
    "\n",
    "        return in_signal + awg_noise\n",
    "\n",
    "    def sv_decomp(self, rx_sig):\n",
    "        \"\"\"\n",
    "        Perform SVD for the matrix in each sub-band\n",
    "        :param rx_sig: Channel matrix at the receiver in each sub-band\n",
    "        :return lsv, sval, rsv: Left, Right Singular Vectors and Singular Values for the matrix in each sub-band\n",
    "        \"\"\"\n",
    "        lsv = zeros(self.num_subbands, dtype=object)\n",
    "        sval = zeros(self.num_subbands, dtype=object)\n",
    "        rsv = zeros(self.num_subbands, dtype=object)\n",
    "\n",
    "        for sb in range(0, self.num_subbands):\n",
    "            U, S, VH = svd(rx_sig[sb])\n",
    "            V = conj(VH).T\n",
    "            ph_shift_u = diag(exp(-1j * angle(U[0, :])))\n",
    "            ph_shift_v = diag(exp(-1j * angle(V[0, :])))\n",
    "            lsv[sb] = dot(U, ph_shift_u)\n",
    "            sval[sb] = S\n",
    "            rsv[sb] = dot(V, ph_shift_v)\n",
    "\n",
    "        return lsv, sval, rsv\n",
    "\n",
    "    def receive(self, *args):\n",
    "        \"\"\"\n",
    "        Contains 3 cases for the 3 steps of the process depending who is the receiver (Alice or Bob)\n",
    "        Generates the frequency domain rx signal in each sub-band which is of the form H*G*F\n",
    "        H - channel, G - random unitary or LSV from SVD, F - DFT precoder\n",
    "        :param args: 0 - who is receiving, 1 - signal to noise ratio in dB, 2 - freq domain channel,\n",
    "        3 - random unitary or LSV from SVD, 4 - DFT precoder\n",
    "        :return: frequency domain rx signal in each sub-band\n",
    "        \"\"\"\n",
    "        rx_node = args[0]\n",
    "        SNRdB = args[1]\n",
    "        if rx_node == 'Bob' and len(args) == 4:\n",
    "                HAB = args[2]\n",
    "                GA = args[3]\n",
    "                rx_sigB = zeros(self.num_subbands, dtype=object)\n",
    "                for sb in range(0, self.num_subbands):\n",
    "                    tx_sig = multi_dot([HAB[sb], GA[sb]])\n",
    "                    # rx_sigB[sb] = tx_sig\n",
    "                    rx_sigB[sb] = self.awgn(tx_sig, SNRdB)\n",
    "                return rx_sigB\n",
    "        if rx_node == 'Bob' and len(args) == 5:\n",
    "            HAB = args[2]\n",
    "            UA = args[3]\n",
    "            FA = args[4]\n",
    "            rx_sigB = zeros(self.num_subbands, dtype=object)\n",
    "            for sb in range(0, self.num_subbands):\n",
    "                tx_sig = multi_dot([HAB[sb], conj(UA[sb]), conj(FA[sb]).T])\n",
    "                rx_sigB[sb] = self.awgn(tx_sig, SNRdB)\n",
    "            return rx_sigB\n",
    "        elif rx_node == 'Alice' and len(args) == 5:\n",
    "            HBA = args[2]\n",
    "            UB = args[3]\n",
    "            FB = args[4]\n",
    "            rx_sigA = zeros(self.num_subbands, dtype=object)\n",
    "            for sb in range(0, self.num_subbands):\n",
    "                tx_sig = multi_dot([HBA[sb], conj(UB[sb]), conj(FB[sb]).T])\n",
    "                rx_sigA[sb] = self.awgn(tx_sig, SNRdB)\n",
    "                # rx_sigA[sb] = tx_sig\n",
    "            return rx_sigA\n",
    "\n",
    "\n",
    "    def secret_key_gen(self):\n",
    "        \"\"\"\n",
    "        Generate private info bits in each sub-band\n",
    "        :return bits_subband: private info bits in each sub-band\n",
    "        \"\"\"\n",
    "        bits_subband = zeros(self.num_subbands, dtype=object)\n",
    "\n",
    "        secret_key = randint(0, 2, self.key_len)\n",
    "\n",
    "        # Map secret key to subbands\n",
    "        for sb in range(self.num_subbands):\n",
    "            start = sb * self.bit_codebook\n",
    "            fin = start + self.bit_codebook\n",
    "\n",
    "            bits_subband[sb] = secret_key[start: fin]\n",
    "\n",
    "        return bits_subband\n",
    "\n",
    "    def precoder_select(self, bits_subband, codebook):\n",
    "        \"\"\"\n",
    "        selects the DFT precoder from the DFT codebook based. Bits are converted to decimal and used as look up index.\n",
    "        :param bits_subband: Bits in each sub-band\n",
    "        :param codebook: DFT codebook of matrix precoders\n",
    "        :return precoder: Selected DFT preocder from codebook for each sub-band\n",
    "        \"\"\"\n",
    "        precoder = zeros(self.num_subbands, dtype=object)\n",
    "\n",
    "        for sb in range(self.num_subbands):\n",
    "            bits = bits_subband[sb]\n",
    "            start = self.bit_codebook - 1\n",
    "            bi2dec_wts = 2**(array(range(start, -1, -1)))\n",
    "            codebook_index = sum(bits*bi2dec_wts)\n",
    "            precoder[sb] = codebook[codebook_index]\n",
    "\n",
    "        return precoder\n",
    "    @staticmethod\n",
    "    def dec2binary(x, num_bits):\n",
    "        \"\"\"\n",
    "        Covert decimal number to binary array of ints (1s and 0s)\n",
    "        :param x: input decimal number\n",
    "        :param num_bits: Number bits required in the binary format\n",
    "        :return bits: binary array of ints (1s and 0s)\n",
    "        \"\"\"\n",
    "        bit_str = [char for char in format(x[0, 0], '0' + str(num_bits) + 'b')]\n",
    "        bits = array([int(char) for char in bit_str])\n",
    "        # print(x[0, 0], bits)\n",
    "        return bits\n",
    "\n",
    "    def PMI_estimate(self, rx_precoder, codebook):\n",
    "        \"\"\"\n",
    "        Apply minumum distance to estimate the transmitted precoder, its index in the codebook and the binary equivalent\n",
    "        of the index\n",
    "        :param rx_precoder: observed precoder (RSV of SVD)\n",
    "        :param codebook: DFT codebook of matrix precoders\n",
    "        :return PMI_sb_estimate, bits_sb_estimate: Preocder matrix index and bits for each sub-band\n",
    "        \"\"\"\n",
    "        PMI_sb_estimate = zeros(self.num_subbands, dtype=int)\n",
    "        bits_sb_estimate = zeros(self.num_subbands, dtype=object)\n",
    "\n",
    "        for sb in range(self.num_subbands):\n",
    "            dist = zeros(len(codebook), dtype=float)\n",
    "\n",
    "            for prec in range(len(codebook)):\n",
    "                diff = rx_precoder[sb] - codebook[prec]\n",
    "                diff_squared = real(diff*conj(diff))\n",
    "                dist[prec] = sqrt(diff_squared.sum())\n",
    "            min_dist = min(dist)\n",
    "            PMI_estimate = argwhere(dist == min_dist)\n",
    "            PMI_sb_estimate[sb] = PMI_estimate\n",
    "            bits_sb_estimate[sb] = self.dec2binary(PMI_estimate, self.bit_codebook)\n",
    "\n",
    "        return PMI_sb_estimate, bits_sb_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSParameters:\n",
    "\n",
    "    def __init__(self, prof):\n",
    "        \"\"\"\n",
    "        Initialization of class\n",
    "        :param prof: PLS profile containing basic parameters such as bandwidth, antennas bin spacing, bits in codebook\n",
    "        index\n",
    "        \"\"\"\n",
    "        self.bandwidth = prof['bandwidth']\n",
    "        self.bin_spacing = prof['bin_spacing']\n",
    "        self.num_ant = prof['num_ant']\n",
    "        self.bit_codebook = prof['bit_codebook']\n",
    "\n",
    "        self.NFFT = int(floor(self.bandwidth/self.bin_spacing))\n",
    "        self.num_used_bins = self.NFFT - 2\n",
    "        self.subband_size = self.num_ant\n",
    "\n",
    "        self.num_subbands = int(floor(self.num_used_bins/self.subband_size))\n",
    "        self.num_PMI = self.num_subbands\n",
    "\n",
    "    def codebook_gen(self):\n",
    "        \"\"\"\n",
    "        Generate DFT codebbok of matrix preocders\n",
    "        :return: matrix of matrix preocders\n",
    "        \"\"\"\n",
    "        num_precoders = 2**self.bit_codebook\n",
    "        codebook = zeros(num_precoders, dtype=object)\n",
    "\n",
    "        for p in range(0, num_precoders):\n",
    "            precoder = zeros((self.num_ant, self.num_ant), dtype=complex)\n",
    "            for m in range(0, self.num_ant):\n",
    "                for n in range(0, self.num_ant):\n",
    "                    w = exp(1j*2*pi*(n/self.num_ant)*(m + p/num_precoders))\n",
    "                    precoder[n, m] = (1/sqrt(self.num_ant))*w\n",
    "\n",
    "            codebook[p] = precoder\n",
    "\n",
    "        return codebook\n",
    "\n",
    "    def channel_gen(self):\n",
    "        \"\"\"\n",
    "        Generate generic Rayleigh fading channels in the frequency domain\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        HAB = zeros(self.num_subbands, dtype=object)\n",
    "        HBA = zeros(self.num_subbands, dtype=object)\n",
    "\n",
    "        for sb in range(0, self.num_subbands):\n",
    "            H = 1/sqrt(2)*(normal(0, 1, (self.num_ant, self.num_ant)) + 1j*normal(0, 1, (self.num_ant, self.num_ant)))\n",
    "            HAB[sb] = H\n",
    "            HBA[sb] = H.T\n",
    "\n",
    "        return HAB, HBA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_array2dec(bin_array):\n",
    "    arr_reversed = bin_array[::-1]\n",
    "    dec = 0\n",
    "    for j in range(len(arr_reversed)):\n",
    "        dec += (2 ** j) * arr_reversed[j]\n",
    "    return dec\n",
    "\n",
    "def crop_center(img,cropx,cropy):\n",
    "    y,x = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "def get_img_from_fig(fig, dpi=180):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    img_arr = frombuffer(buf.getvalue(), dtype=uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    scaled_image = cv2.resize(img, (100, 100))  \n",
    "#     print(scaled_image.shape)\n",
    "    \n",
    "    cropped_img = crop_center(scaled_image, 70, 70)\n",
    "    cropped_img = cv2.bitwise_not(cropped_img)\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_SNR = 20\n",
    "# SNR_dB = range(0, max_SNR, 5)\n",
    "SNR_dB = [1000]\n",
    "# SNR_dB = [45, 45]\n",
    "max_iter = 10\n",
    "\n",
    "pls_profiles = {\n",
    "               0: {'bandwidth': 960e3, 'bin_spacing': 15e3, 'num_ant': 2, 'bit_codebook': 2},\n",
    "               }\n",
    "\n",
    "# dbg = 1\n",
    "# for prof in pls_profiles.keys():\n",
    "#     df = pd.DataFrame(list(pls_profiles[prof].items()),columns = ['column 1', 'column 2'])\n",
    "# dbg = 1\n",
    "df = pd.DataFrame(columns = ['Bandwidth', 'Bin Spacing', 'Antennas',\n",
    "                             'Bit Codebook', 'SNR', 'Obs Precoder', 'Correct PMI'])\n",
    "dbg = 1\n",
    "\n",
    "for prof in pls_profiles.values():\n",
    "    pls_params = PLSParameters(prof)\n",
    "    codebook = pls_params.codebook_gen()\n",
    "    \n",
    "    N = Node(pls_params)  # Wireless network node - could be Alice or Bob\n",
    "\n",
    "    for s in range(len(SNR_dB)):\n",
    " \n",
    "        for i in range(max_iter):\n",
    "            HAB, HBA = pls_params.channel_gen()\n",
    "\n",
    "            ## 1. Alice to Bob\n",
    "            GA = N.unitary_gen()\n",
    "            rx_sigB0 = N.receive('Bob', SNR_dB[s], HAB, GA)\n",
    "\n",
    "            ## 1. At Bob\n",
    "            UB0 = N.sv_decomp(rx_sigB0)[0]\n",
    "            bits_subbandB = N.secret_key_gen()\n",
    "            transmitted_PMI = bin_array2dec(bits_subbandB[0])\n",
    "\n",
    "            FB = N.precoder_select(bits_subbandB, codebook)\n",
    "\n",
    "            ## 2. Bob to Alice\n",
    "            rx_sigA = N.receive('Alice', SNR_dB[s], HBA, UB0, FB)\n",
    "\n",
    "            ## 2. At Alice\n",
    "            UA, _, VA = N.sv_decomp(rx_sigA)\n",
    "            \n",
    "\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(VA[0].real, VA[0].imag, 'o', color='black')\n",
    "            plt.xlim((-1, 1))\n",
    "            plt.ylim((-1, 1))\n",
    "            plt.close(fig)\n",
    "            plot_img_np = get_img_from_fig(fig)\n",
    "#             plt.imshow(plot_img_np)\n",
    "#             plt.show()\n",
    "            df = df.append({'Bandwidth': pls_params.bandwidth,\n",
    "                            'Bin Spacing': pls_params.bin_spacing,\n",
    "                            'Antennas': pls_params.num_ant,\n",
    "                             'Bit Codebook': pls_params.bit_codebook,\n",
    "                            'SNR': SNR_dB[s],\n",
    "                            'Obs Precoder': plot_img_np,\n",
    "                            'Correct PMI': transmitted_PMI},\n",
    "                           ignore_index=True)\n",
    "    \n",
    "#             print(plot_img_np.shape)\n",
    "        \n",
    "            dbg = 1\n",
    "\n",
    "\n",
    "dbg = 1\n",
    "\n",
    "\n",
    "num_images = df.shape[0]\n",
    "img_arrays = [df['Obs Precoder'][i] for i in range(num_images)]\n",
    "precoder_img_data = stack(img_arrays, axis=0)\n",
    "precoder_labels = array(df['Correct PMI'].tolist())\n",
    "# print(precoder_img_data.shape)\n",
    "\n",
    "# for i in range(num_images):\n",
    "#     plt.imshow(precoder_img_data[i, :, :], cmap = plt.cm.gray)\n",
    "#     plt.show()\n",
    "\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.head())\n",
    "# print(df['Obs Precoder'][0])\n",
    "# print(df['Obs Precoder'][5].shape)\n",
    "# plt.imshow(df['Obs Precoder'][5],  cmap = plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecoderDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "precoder_dataset = PrecoderDataset(precoder_img_data, precoder_labels)\n",
    "train, val = random_split(precoder_dataset, [int(0.8*max_iter), int(0.2*max_iter)])\n",
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "# increase batch size\n",
    "val_loader = DataLoader(\n",
    "    val,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Batch idx 0, data shape torch.Size([8, 70, 70]), target shape torch.Size([8])\n",
      "Val: Batch idx 0, data shape torch.Size([2, 70, 70]), target shape torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print('Train: Batch idx {}, data shape {}, target shape {}'.format(\n",
    "        batch_idx, data.shape, target.shape))\n",
    "    \n",
    "for batch_idx, (data, target) in enumerate(val_loader):\n",
    "    print('Val: Batch idx {}, data shape {}, target shape {}'.format(\n",
    "        batch_idx, data.shape, target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "from torch import optim \n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "case = 0 # training for the first pls profile\n",
    "num_classes = 2**pls_profiles[case]['bit_codebook']\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#         nn.Linear(70*70, 2000),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(2000, 1000),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(1000, 64),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(64, num_classes),\n",
    "#         nn.Softmax(),\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(70*70, 1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, num_classes),\n",
    "        nn.Softmax(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# if use_cuda:\n",
    "#     model.cuda()\n",
    "\n",
    "# summary(model, (70, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utsa/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 1.35\n",
      "Epoch 2, training loss: 0.89\n",
      "Epoch 3, training loss: 0.87\n",
      "Epoch 4, training loss: 0.87\n",
      "Epoch 5, training loss: 0.87\n",
      "Epoch 6, training loss: 0.87\n",
      "Epoch 7, training loss: 0.87\n",
      "Epoch 8, training loss: 0.87\n",
      "Epoch 9, training loss: 0.87\n",
      "Epoch 10, training loss: 0.87\n",
      "Epoch 11, training loss: 0.87\n",
      "Epoch 12, training loss: 0.87\n",
      "Epoch 13, training loss: 0.87\n",
      "Epoch 14, training loss: 0.87\n",
      "Epoch 15, training loss: 0.87\n",
      "Epoch 16, training loss: 0.87\n",
      "Epoch 17, training loss: 0.87\n",
      "Epoch 18, training loss: 0.87\n",
      "Epoch 19, training loss: 0.87\n",
      "Epoch 20, training loss: 0.87\n",
      "Epoch 21, training loss: 0.87\n",
      "Epoch 22, training loss: 0.87\n",
      "Epoch 23, training loss: 0.87\n",
      "Epoch 24, training loss: 0.87\n",
      "Epoch 25, training loss: 0.87\n",
      "Epoch 26, training loss: 0.87\n",
      "Epoch 27, training loss: 0.87\n",
      "Epoch 28, training loss: 0.87\n",
      "Epoch 29, training loss: 0.87\n",
      "Epoch 30, training loss: 0.87\n",
      "Epoch 31, training loss: 0.87\n",
      "Epoch 32, training loss: 0.87\n",
      "Epoch 33, training loss: 0.87\n",
      "Epoch 34, training loss: 0.87\n",
      "Epoch 35, training loss: 0.87\n",
      "Epoch 36, training loss: 0.87\n",
      "Epoch 37, training loss: 0.87\n",
      "Epoch 38, training loss: 0.87\n",
      "Epoch 39, training loss: 0.87\n",
      "Epoch 40, training loss: 0.87\n",
      "Epoch 41, training loss: 0.87\n",
      "Epoch 42, training loss: 0.87\n",
      "Epoch 43, training loss: 0.87\n",
      "Epoch 44, training loss: 0.87\n",
      "Epoch 45, training loss: 0.87\n",
      "Epoch 46, training loss: 0.87\n",
      "Epoch 47, training loss: 0.87\n",
      "Epoch 48, training loss: 0.87\n",
      "Epoch 49, training loss: 0.87\n",
      "Epoch 50, training loss: 0.87\n",
      "Epoch 51, training loss: 0.87\n",
      "Epoch 52, training loss: 0.87\n",
      "Epoch 53, training loss: 0.87\n",
      "Epoch 54, training loss: 0.87\n",
      "Epoch 55, training loss: 1.12\n",
      "Epoch 56, training loss: 1.12\n",
      "Epoch 57, training loss: 1.12\n",
      "Epoch 58, training loss: 1.12\n",
      "Epoch 59, training loss: 1.12\n",
      "Epoch 60, training loss: 1.12\n",
      "Epoch 61, training loss: 1.12\n",
      "Epoch 62, training loss: 1.12\n",
      "Epoch 63, training loss: 1.12\n",
      "Epoch 64, training loss: 1.12\n",
      "Epoch 65, training loss: 1.12\n",
      "Epoch 66, training loss: 1.12\n",
      "Epoch 67, training loss: 1.12\n",
      "Epoch 68, training loss: 1.12\n",
      "Epoch 69, training loss: 1.12\n",
      "Epoch 70, training loss: 1.12\n",
      "Epoch 71, training loss: 1.12\n",
      "Epoch 72, training loss: 1.12\n",
      "Epoch 73, training loss: 1.12\n",
      "Epoch 74, training loss: 1.12\n",
      "Epoch 75, training loss: 1.12\n",
      "Epoch 76, training loss: 1.12\n",
      "Epoch 77, training loss: 1.12\n",
      "Epoch 78, training loss: 1.12\n",
      "Epoch 79, training loss: 1.12\n",
      "Epoch 80, training loss: 1.12\n",
      "Epoch 81, training loss: 1.12\n",
      "Epoch 82, training loss: 1.12\n",
      "Epoch 83, training loss: 1.12\n",
      "Epoch 84, training loss: 1.12\n",
      "Epoch 85, training loss: 1.12\n",
      "Epoch 86, training loss: 1.12\n",
      "Epoch 87, training loss: 1.12\n",
      "Epoch 88, training loss: 1.12\n",
      "Epoch 89, training loss: 1.12\n",
      "Epoch 90, training loss: 1.12\n",
      "Epoch 91, training loss: 1.12\n",
      "Epoch 92, training loss: 1.12\n",
      "Epoch 93, training loss: 1.12\n",
      "Epoch 94, training loss: 1.12\n",
      "Epoch 95, training loss: 1.12\n",
      "Epoch 96, training loss: 1.12\n",
      "Epoch 97, training loss: 1.12\n",
      "Epoch 98, training loss: 1.12\n",
      "Epoch 99, training loss: 1.12\n",
      "Epoch 100, training loss: 1.12\n",
      "Epoch 101, training loss: 1.12\n",
      "Epoch 102, training loss: 1.12\n",
      "Epoch 103, training loss: 1.12\n",
      "Epoch 104, training loss: 1.12\n",
      "Epoch 105, training loss: 1.12\n",
      "Epoch 106, training loss: 1.12\n",
      "Epoch 107, training loss: 1.12\n",
      "Epoch 108, training loss: 1.12\n",
      "Epoch 109, training loss: 1.12\n",
      "Epoch 110, training loss: 1.12\n",
      "Epoch 111, training loss: 1.12\n",
      "Epoch 112, training loss: 1.12\n",
      "Epoch 113, training loss: 1.12\n",
      "Epoch 114, training loss: 1.12\n",
      "Epoch 115, training loss: 1.12\n",
      "Epoch 116, training loss: 1.12\n",
      "Epoch 117, training loss: 1.12\n",
      "Epoch 118, training loss: 1.12\n",
      "Epoch 119, training loss: 1.12\n",
      "Epoch 120, training loss: 1.12\n",
      "Epoch 121, training loss: 1.12\n",
      "Epoch 122, training loss: 1.12\n",
      "Epoch 123, training loss: 1.12\n",
      "Epoch 124, training loss: 1.12\n",
      "Epoch 125, training loss: 1.12\n",
      "Epoch 126, training loss: 1.12\n",
      "Epoch 127, training loss: 1.12\n",
      "Epoch 128, training loss: 1.12\n",
      "Epoch 129, training loss: 1.12\n",
      "Epoch 130, training loss: 1.12\n",
      "Epoch 131, training loss: 1.12\n",
      "Epoch 132, training loss: 1.12\n",
      "Epoch 133, training loss: 1.12\n",
      "Epoch 134, training loss: 1.12\n",
      "Epoch 135, training loss: 1.12\n",
      "Epoch 136, training loss: 1.12\n",
      "Epoch 137, training loss: 1.12\n",
      "Epoch 138, training loss: 1.12\n",
      "Epoch 139, training loss: 1.12\n",
      "Epoch 140, training loss: 1.12\n",
      "Epoch 141, training loss: 1.12\n",
      "Epoch 142, training loss: 1.12\n",
      "Epoch 143, training loss: 1.12\n",
      "Epoch 144, training loss: 1.12\n",
      "Epoch 145, training loss: 1.12\n",
      "Epoch 146, training loss: 1.12\n",
      "Epoch 147, training loss: 1.12\n",
      "Epoch 148, training loss: 1.12\n",
      "Epoch 149, training loss: 1.12\n",
      "Epoch 150, training loss: 1.12\n",
      "Epoch 151, training loss: 1.12\n",
      "Epoch 152, training loss: 1.12\n",
      "Epoch 153, training loss: 1.12\n",
      "Epoch 154, training loss: 1.12\n",
      "Epoch 155, training loss: 1.12\n",
      "Epoch 156, training loss: 1.12\n",
      "Epoch 157, training loss: 1.12\n",
      "Epoch 158, training loss: 1.12\n",
      "Epoch 159, training loss: 1.12\n",
      "Epoch 160, training loss: 1.12\n",
      "Epoch 161, training loss: 1.12\n",
      "Epoch 162, training loss: 1.12\n",
      "Epoch 163, training loss: 1.12\n",
      "Epoch 164, training loss: 1.12\n",
      "Epoch 165, training loss: 1.12\n",
      "Epoch 166, training loss: 1.12\n",
      "Epoch 167, training loss: 1.12\n",
      "Epoch 168, training loss: 1.12\n",
      "Epoch 169, training loss: 1.12\n",
      "Epoch 170, training loss: 1.12\n",
      "Epoch 171, training loss: 1.12\n",
      "Epoch 172, training loss: 1.12\n",
      "Epoch 173, training loss: 1.12\n",
      "Epoch 174, training loss: 1.12\n",
      "Epoch 175, training loss: 1.12\n",
      "Epoch 176, training loss: 1.12\n",
      "Epoch 177, training loss: 1.12\n",
      "Epoch 178, training loss: 1.12\n",
      "Epoch 179, training loss: 1.12\n",
      "Epoch 180, training loss: 1.12\n",
      "Epoch 181, training loss: 1.12\n",
      "Epoch 182, training loss: 1.12\n",
      "Epoch 183, training loss: 1.12\n",
      "Epoch 184, training loss: 1.12\n",
      "Epoch 185, training loss: 1.12\n",
      "Epoch 186, training loss: 1.12\n",
      "Epoch 187, training loss: 1.12\n",
      "Epoch 188, training loss: 1.12\n",
      "Epoch 189, training loss: 1.12\n",
      "Epoch 190, training loss: 1.12\n",
      "Epoch 191, training loss: 1.12\n",
      "Epoch 192, training loss: 1.12\n",
      "Epoch 193, training loss: 1.12\n",
      "Epoch 194, training loss: 1.12\n",
      "Epoch 195, training loss: 1.12\n",
      "Epoch 196, training loss: 1.12\n",
      "Epoch 197, training loss: 1.12\n",
      "Epoch 198, training loss: 1.12\n",
      "Epoch 199, training loss: 1.12\n",
      "Epoch 200, training loss: 1.12\n",
      "Epoch 201, training loss: 1.12\n",
      "Epoch 202, training loss: 1.12\n",
      "Epoch 203, training loss: 1.12\n",
      "Epoch 204, training loss: 1.12\n",
      "Epoch 205, training loss: 1.12\n",
      "Epoch 206, training loss: 1.12\n",
      "Epoch 207, training loss: 1.12\n",
      "Epoch 208, training loss: 1.12\n",
      "Epoch 209, training loss: 1.12\n",
      "Epoch 210, training loss: 1.12\n",
      "Epoch 211, training loss: 1.12\n",
      "Epoch 212, training loss: 1.12\n",
      "Epoch 213, training loss: 1.12\n",
      "Epoch 214, training loss: 1.12\n",
      "Epoch 215, training loss: 1.12\n",
      "Epoch 216, training loss: 1.12\n",
      "Epoch 217, training loss: 1.12\n",
      "Epoch 218, training loss: 1.12\n",
      "Epoch 219, training loss: 1.12\n",
      "Epoch 220, training loss: 1.12\n",
      "Epoch 221, training loss: 1.12\n",
      "Epoch 222, training loss: 1.12\n",
      "Epoch 223, training loss: 1.12\n",
      "Epoch 224, training loss: 1.12\n",
      "Epoch 225, training loss: 1.12\n",
      "Epoch 226, training loss: 1.12\n",
      "Epoch 227, training loss: 1.12\n",
      "Epoch 228, training loss: 1.12\n",
      "Epoch 229, training loss: 1.12\n",
      "Epoch 230, training loss: 1.12\n",
      "Epoch 231, training loss: 1.12\n",
      "Epoch 232, training loss: 1.12\n",
      "Epoch 233, training loss: 1.12\n",
      "Epoch 234, training loss: 1.12\n",
      "Epoch 235, training loss: 1.12\n",
      "Epoch 236, training loss: 1.12\n",
      "Epoch 237, training loss: 1.12\n",
      "Epoch 238, training loss: 1.12\n",
      "Epoch 239, training loss: 1.12\n",
      "Epoch 240, training loss: 1.12\n",
      "Epoch 241, training loss: 1.12\n",
      "Epoch 242, training loss: 1.12\n",
      "Epoch 243, training loss: 1.12\n",
      "Epoch 244, training loss: 1.12\n",
      "Epoch 245, training loss: 1.12\n",
      "Epoch 246, training loss: 1.12\n",
      "Epoch 247, training loss: 1.12\n",
      "Epoch 248, training loss: 1.12\n",
      "Epoch 249, training loss: 1.12\n",
      "Epoch 250, training loss: 1.12\n",
      "Epoch 251, training loss: 1.12\n",
      "Epoch 252, training loss: 1.12\n",
      "Epoch 253, training loss: 1.12\n",
      "Epoch 254, training loss: 1.12\n",
      "Epoch 255, training loss: 1.12\n",
      "Epoch 256, training loss: 1.12\n",
      "Epoch 257, training loss: 1.12\n",
      "Epoch 258, training loss: 1.12\n",
      "Epoch 259, training loss: 1.12\n",
      "Epoch 260, training loss: 1.12\n",
      "Epoch 261, training loss: 1.12\n",
      "Epoch 262, training loss: 1.12\n",
      "Epoch 263, training loss: 1.12\n",
      "Epoch 264, training loss: 1.12\n",
      "Epoch 265, training loss: 1.12\n",
      "Epoch 266, training loss: 1.12\n",
      "Epoch 267, training loss: 1.12\n",
      "Epoch 268, training loss: 1.12\n",
      "Epoch 269, training loss: 1.12\n",
      "Epoch 270, training loss: 1.12\n",
      "Epoch 271, training loss: 1.12\n",
      "Epoch 272, training loss: 1.12\n",
      "Epoch 273, training loss: 1.12\n",
      "Epoch 274, training loss: 1.12\n",
      "Epoch 275, training loss: 1.12\n",
      "Epoch 276, training loss: 1.12\n",
      "Epoch 277, training loss: 1.12\n",
      "Epoch 278, training loss: 1.12\n",
      "Epoch 279, training loss: 1.12\n",
      "Epoch 280, training loss: 1.12\n",
      "Epoch 281, training loss: 1.12\n",
      "Epoch 282, training loss: 1.12\n",
      "Epoch 283, training loss: 1.12\n",
      "Epoch 284, training loss: 1.12\n",
      "Epoch 285, training loss: 1.12\n",
      "Epoch 286, training loss: 1.12\n",
      "Epoch 287, training loss: 1.12\n",
      "Epoch 288, training loss: 1.12\n",
      "Epoch 289, training loss: 1.12\n",
      "Epoch 290, training loss: 1.12\n",
      "Epoch 291, training loss: 1.12\n",
      "Epoch 292, training loss: 1.12\n",
      "Epoch 293, training loss: 1.12\n",
      "Epoch 294, training loss: 1.12\n",
      "Epoch 295, training loss: 1.12\n",
      "Epoch 296, training loss: 1.12\n",
      "Epoch 297, training loss: 1.12\n",
      "Epoch 298, training loss: 1.12\n",
      "Epoch 299, training loss: 1.12\n",
      "Epoch 300, training loss: 1.12\n",
      "Epoch 301, training loss: 1.12\n",
      "Epoch 302, training loss: 1.12\n",
      "Epoch 303, training loss: 1.12\n",
      "Epoch 304, training loss: 1.12\n",
      "Epoch 305, training loss: 1.12\n",
      "Epoch 306, training loss: 1.12\n",
      "Epoch 307, training loss: 1.12\n",
      "Epoch 308, training loss: 1.12\n",
      "Epoch 309, training loss: 1.12\n",
      "Epoch 310, training loss: 1.12\n",
      "Epoch 311, training loss: 1.12\n",
      "Epoch 312, training loss: 1.12\n",
      "Epoch 313, training loss: 1.12\n",
      "Epoch 314, training loss: 1.12\n",
      "Epoch 315, training loss: 1.12\n",
      "Epoch 316, training loss: 1.12\n",
      "Epoch 317, training loss: 1.12\n",
      "Epoch 318, training loss: 1.12\n",
      "Epoch 319, training loss: 1.12\n",
      "Epoch 320, training loss: 1.12\n",
      "Epoch 321, training loss: 1.12\n",
      "Epoch 322, training loss: 1.12\n",
      "Epoch 323, training loss: 1.12\n",
      "Epoch 324, training loss: 1.12\n",
      "Epoch 325, training loss: 1.12\n",
      "Epoch 326, training loss: 1.12\n",
      "Epoch 327, training loss: 1.12\n",
      "Epoch 328, training loss: 1.12\n",
      "Epoch 329, training loss: 1.12\n",
      "Epoch 330, training loss: 1.12\n",
      "Epoch 331, training loss: 1.12\n",
      "Epoch 332, training loss: 1.12\n",
      "Epoch 333, training loss: 1.12\n",
      "Epoch 334, training loss: 1.12\n",
      "Epoch 335, training loss: 1.12\n",
      "Epoch 336, training loss: 1.12\n",
      "Epoch 337, training loss: 1.12\n",
      "Epoch 338, training loss: 1.12\n",
      "Epoch 339, training loss: 1.12\n",
      "Epoch 340, training loss: 1.12\n",
      "Epoch 341, training loss: 1.12\n",
      "Epoch 342, training loss: 1.12\n",
      "Epoch 343, training loss: 1.12\n",
      "Epoch 344, training loss: 1.12\n",
      "Epoch 345, training loss: 1.12\n",
      "Epoch 346, training loss: 1.12\n",
      "Epoch 347, training loss: 1.12\n",
      "Epoch 348, training loss: 1.12\n",
      "Epoch 349, training loss: 1.12\n",
      "Epoch 350, training loss: 1.12\n",
      "Epoch 351, training loss: 1.12\n",
      "Epoch 352, training loss: 1.12\n",
      "Epoch 353, training loss: 1.12\n",
      "Epoch 354, training loss: 1.12\n",
      "Epoch 355, training loss: 1.12\n",
      "Epoch 356, training loss: 1.12\n",
      "Epoch 357, training loss: 1.12\n",
      "Epoch 358, training loss: 1.12\n",
      "Epoch 359, training loss: 1.12\n",
      "Epoch 360, training loss: 1.12\n",
      "Epoch 361, training loss: 1.12\n",
      "Epoch 362, training loss: 1.12\n",
      "Epoch 363, training loss: 1.12\n",
      "Epoch 364, training loss: 1.12\n",
      "Epoch 365, training loss: 1.12\n",
      "Epoch 366, training loss: 1.12\n",
      "Epoch 367, training loss: 1.12\n",
      "Epoch 368, training loss: 1.12\n",
      "Epoch 369, training loss: 1.12\n",
      "Epoch 370, training loss: 1.12\n",
      "Epoch 371, training loss: 1.12\n",
      "Epoch 372, training loss: 1.12\n",
      "Epoch 373, training loss: 1.12\n",
      "Epoch 374, training loss: 1.12\n",
      "Epoch 375, training loss: 1.12\n",
      "Epoch 376, training loss: 1.12\n",
      "Epoch 377, training loss: 1.12\n",
      "Epoch 378, training loss: 1.12\n",
      "Epoch 379, training loss: 1.12\n",
      "Epoch 380, training loss: 1.12\n",
      "Epoch 381, training loss: 1.12\n",
      "Epoch 382, training loss: 1.12\n",
      "Epoch 383, training loss: 1.12\n",
      "Epoch 384, training loss: 1.12\n",
      "Epoch 385, training loss: 1.12\n",
      "Epoch 386, training loss: 1.12\n",
      "Epoch 387, training loss: 1.12\n",
      "Epoch 388, training loss: 1.12\n",
      "Epoch 389, training loss: 1.12\n",
      "Epoch 390, training loss: 1.12\n",
      "Epoch 391, training loss: 1.12\n",
      "Epoch 392, training loss: 1.12\n",
      "Epoch 393, training loss: 1.12\n",
      "Epoch 394, training loss: 1.12\n",
      "Epoch 395, training loss: 1.12\n",
      "Epoch 396, training loss: 1.12\n",
      "Epoch 397, training loss: 1.12\n",
      "Epoch 398, training loss: 1.12\n",
      "Epoch 399, training loss: 1.12\n",
      "Epoch 400, training loss: 1.12\n",
      "Epoch 401, training loss: 1.12\n",
      "Epoch 402, training loss: 1.12\n",
      "Epoch 403, training loss: 1.12\n",
      "Epoch 404, training loss: 1.12\n",
      "Epoch 405, training loss: 1.12\n",
      "Epoch 406, training loss: 1.12\n",
      "Epoch 407, training loss: 1.12\n",
      "Epoch 408, training loss: 1.12\n",
      "Epoch 409, training loss: 1.12\n",
      "Epoch 410, training loss: 1.12\n",
      "Epoch 411, training loss: 1.12\n",
      "Epoch 412, training loss: 1.12\n",
      "Epoch 413, training loss: 1.12\n",
      "Epoch 414, training loss: 1.12\n",
      "Epoch 415, training loss: 1.12\n",
      "Epoch 416, training loss: 1.12\n",
      "Epoch 417, training loss: 1.12\n",
      "Epoch 418, training loss: 1.12\n",
      "Epoch 419, training loss: 1.12\n",
      "Epoch 420, training loss: 1.12\n",
      "Epoch 421, training loss: 1.12\n",
      "Epoch 422, training loss: 1.12\n",
      "Epoch 423, training loss: 1.12\n",
      "Epoch 424, training loss: 1.12\n",
      "Epoch 425, training loss: 1.12\n",
      "Epoch 426, training loss: 1.12\n",
      "Epoch 427, training loss: 1.12\n",
      "Epoch 428, training loss: 1.12\n",
      "Epoch 429, training loss: 1.12\n",
      "Epoch 430, training loss: 1.12\n",
      "Epoch 431, training loss: 1.12\n",
      "Epoch 432, training loss: 1.12\n",
      "Epoch 433, training loss: 1.12\n",
      "Epoch 434, training loss: 1.12\n",
      "Epoch 435, training loss: 1.12\n",
      "Epoch 436, training loss: 1.12\n",
      "Epoch 437, training loss: 1.12\n",
      "Epoch 438, training loss: 1.12\n",
      "Epoch 439, training loss: 1.12\n",
      "Epoch 440, training loss: 1.12\n",
      "Epoch 441, training loss: 1.12\n",
      "Epoch 442, training loss: 1.12\n",
      "Epoch 443, training loss: 1.12\n",
      "Epoch 444, training loss: 1.12\n",
      "Epoch 445, training loss: 1.12\n",
      "Epoch 446, training loss: 1.12\n",
      "Epoch 447, training loss: 1.12\n",
      "Epoch 448, training loss: 1.12\n",
      "Epoch 449, training loss: 1.12\n",
      "Epoch 450, training loss: 1.12\n",
      "Epoch 451, training loss: 1.12\n",
      "Epoch 452, training loss: 1.12\n",
      "Epoch 453, training loss: 1.12\n",
      "Epoch 454, training loss: 1.12\n",
      "Epoch 455, training loss: 1.12\n",
      "Epoch 456, training loss: 1.12\n",
      "Epoch 457, training loss: 1.12\n",
      "Epoch 458, training loss: 1.12\n",
      "Epoch 459, training loss: 1.12\n",
      "Epoch 460, training loss: 1.12\n",
      "Epoch 461, training loss: 1.12\n",
      "Epoch 462, training loss: 1.12\n",
      "Epoch 463, training loss: 1.12\n",
      "Epoch 464, training loss: 1.12\n",
      "Epoch 465, training loss: 1.12\n",
      "Epoch 466, training loss: 1.12\n",
      "Epoch 467, training loss: 1.12\n",
      "Epoch 468, training loss: 1.12\n",
      "Epoch 469, training loss: 1.12\n",
      "Epoch 470, training loss: 1.12\n",
      "Epoch 471, training loss: 1.12\n",
      "Epoch 472, training loss: 1.12\n",
      "Epoch 473, training loss: 1.12\n",
      "Epoch 474, training loss: 1.12\n",
      "Epoch 475, training loss: 1.12\n",
      "Epoch 476, training loss: 1.12\n",
      "Epoch 477, training loss: 1.12\n",
      "Epoch 478, training loss: 1.12\n",
      "Epoch 479, training loss: 1.12\n",
      "Epoch 480, training loss: 1.12\n",
      "Epoch 481, training loss: 1.12\n",
      "Epoch 482, training loss: 1.12\n",
      "Epoch 483, training loss: 1.12\n",
      "Epoch 484, training loss: 1.12\n",
      "Epoch 485, training loss: 1.12\n",
      "Epoch 486, training loss: 1.12\n",
      "Epoch 487, training loss: 1.12\n",
      "Epoch 488, training loss: 1.12\n",
      "Epoch 489, training loss: 1.12\n",
      "Epoch 490, training loss: 1.12\n",
      "Epoch 491, training loss: 1.12\n",
      "Epoch 492, training loss: 1.12\n",
      "Epoch 493, training loss: 1.12\n",
      "Epoch 494, training loss: 1.12\n",
      "Epoch 495, training loss: 1.12\n",
      "Epoch 496, training loss: 1.12\n",
      "Epoch 497, training loss: 1.12\n",
      "Epoch 498, training loss: 1.12\n",
      "Epoch 499, training loss: 1.12\n",
      "Epoch 500, training loss: 1.12\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500 # Number of times you go through the whole dataset\n",
    "for epoch in range(num_epochs):\n",
    "    losses = list()\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        x, y = batch # extracted from the batch \n",
    "        \n",
    "        # x: batch size x 1 channel x height x width \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x.view(batch_size, -1) # -1 means it will mutiply all the dimensions that are not the batch size\n",
    "        # no the size of x will be batch_size rows and height*width columns\n",
    "#         print(x.size())\n",
    "        # step 1: forward pass \n",
    "        prob = model(x) \n",
    "        \n",
    "#         print(prob)\n",
    "#         print(y)\n",
    "        # step 2: compute objective function - measuring distance between the output of the network vs actual answer \n",
    "        obj_func = loss(prob, y)\n",
    "        \n",
    "        # step 3: clear the gradients \n",
    "#         model.zero_grad()\n",
    "        \n",
    "        # step 4: accumulate partial derivatives of obj_func wrt parameters \n",
    "        obj_func.backward()\n",
    "        \n",
    "        # step 5: step in the opposite direction of the gradient \n",
    "        optimizer.step()\n",
    "#         print(obj_func.item())\n",
    "        losses.append(obj_func.item())\n",
    "    print(f'Epoch {epoch + 1}, training loss: {torch.tensor(losses).mean():.2f}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
